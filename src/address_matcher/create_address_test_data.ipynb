{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c8ad96-0286-4ed5-8972-b52e32bbf644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run ./local_dev/spark_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "eeb48701-8d9c-465c-b179-6c0b7fcf3ac2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up reusable functions"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "DESTROY_MODULO = 7\n",
    "DIRTY_MODULO = 2\n",
    "\n",
    "dirty_postcode_mods = [\n",
    "    lambda x: x.replace(\"9\", \"E\"),\n",
    "    lambda x: x.replace(\"E\", \"9\"),\n",
    "    lambda x: x.replace(\"0\", \"o\"),\n",
    "    lambda x: x.replace(\"o\", \"0\"),\n",
    "    lambda x: x.replace(\"1\", \"l\"),\n",
    "    lambda x: x.replace(\"l\", \"1\"),\n",
    "    lambda x: x.replace(\"I\", \"1\"),\n",
    "    lambda x: x.replace(\"B\", \"8\"),\n",
    "    lambda x: x.replace(\"S\", \"5\"),\n",
    "    lambda x: x.replace(\"G\", \"6\"),\n",
    "    lambda x: x.replace(\"Z\", \"2\"),\n",
    "    lambda x: x.replace(\"C\", \"0\"),\n",
    "    lambda x: x.replace(\"K\", \"X\"),\n",
    "    lambda x: x.lower(),\n",
    "    lambda x: x.upper(),\n",
    "    lambda x: x.replace(\" \", \"\"),\n",
    "    lambda x: x[:len(x)//2],\n",
    "    lambda x: x + \"X\",\n",
    "    lambda x: x + \"'\",\n",
    "    lambda x: x + \".\",\n",
    "    lambda x: x + \",\",\n",
    "    lambda x: \"X\" + x,\n",
    "    lambda x: \"]\" + x,\n",
    "    lambda x: x[:-1] + \"Z\" if len(x) > 1 else x,\n",
    "    lambda x: x[1:] if len(x) > 1 else x,\n",
    "    lambda x: x[:-1] if len(x) > 1 else x,\n",
    "    lambda x: x + \" \" + x[-1],\n",
    "    lambda x: x[:2] + \" \" + x[2:] if len(x) > 2 else x,\n",
    "    lambda x: x.replace(\" \", \"  \"),\n",
    "    lambda x: x[::-1],\n",
    "]\n",
    "\n",
    "destroy_postcode_mods = [\n",
    "    lambda x: \"\",\n",
    "    lambda x: \"NOTAPOSTC\",\n",
    "]\n",
    "\n",
    "dirty_address_mods = [\n",
    "    lambda x: x.replace(\"Street\", \"St\"),\n",
    "    lambda x: x.replace(\"Road\", \"Rd\"),\n",
    "    lambda x: x.replace(\"Road\", \"\"),\n",
    "    lambda x: x.replace(\"Street\", \"\"),\n",
    "    lambda x: x.replace(\"o\", \"0\"),\n",
    "    lambda x: x.replace(\"1\", \"l\"),\n",
    "    lambda x: x.replace(\"l\", \"1\"),\n",
    "    lambda x: x.upper(),\n",
    "    lambda x: x.lower(),\n",
    "    lambda x: x + \" Apt 1B\",\n",
    "    lambda x: x[:len(x)//2],\n",
    "    lambda x: x.replace(\"Avenue\", \"Ave\"),\n",
    "    lambda x: x.replace(\"Ave\", \"\"),\n",
    "    lambda x: x.replace(\"Apartment\", \"Apt\"),\n",
    "    lambda x: x.replace(\"Building\", \"Bldg\"),\n",
    "    lambda x: x.replace(\",\", \"\"),\n",
    "    lambda x: x + \", United Kingdom\",\n",
    "    lambda x: x + \", UK\",\n",
    "    lambda x: \"Flat \" + x,\n",
    "    lambda x: x.replace(\" \", \"\"),\n",
    "    lambda x: \" \".join(x.split()[:-1]),\n",
    "    lambda x: \" \".join(x.split()[1:]),\n",
    "    lambda x: x + \" London\",\n",
    "    lambda x: x.replace(\"London\", \"Londom\"),\n",
    "    lambda x: x.replace(\"Main\", \"Mian\"),\n",
    "    lambda x: x.replace(\"o\", \"\"),\n",
    "    lambda x: x.replace(\"a\", \"@\"),\n",
    "    lambda x: x.replace(\"e\", \"3\"),\n",
    "    lambda x: x.replace(\"s\", \"$\"),\n",
    "    lambda x: x.title(),\n",
    "    lambda x: \" \".join(list(x)),\n",
    "]\n",
    "\n",
    "destroy_address_mods = [\n",
    "    lambda x: \"123 Madeup Road, Imaginary Town\",\n",
    "    lambda x: \"\",\n",
    "    lambda x: x[::-1],\n",
    "]\n",
    "\n",
    "def transform_postcode(unique_id, postcode):\n",
    "    if unique_id % DESTROY_MODULO == 0:\n",
    "        return random.choice(destroy_postcode_mods)(postcode)\n",
    "    elif unique_id % DIRTY_MODULO == 0:\n",
    "        return random.choice(dirty_postcode_mods)(postcode)\n",
    "    else:\n",
    "        return postcode\n",
    "\n",
    "def transform_fulladdress(unique_id, fulladdress):\n",
    "    if unique_id % DESTROY_MODULO == 0:\n",
    "        return random.choice(destroy_address_mods)(fulladdress)\n",
    "    elif unique_id % DIRTY_MODULO == 0:\n",
    "        return random.choice(dirty_address_mods)(fulladdress)\n",
    "    else:\n",
    "        return fulladdress\n",
    "\n",
    "def transform_expected_uprn(unique_id, current_uprn):\n",
    "    if unique_id % DESTROY_MODULO == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return current_uprn\n",
    "    \n",
    "transform_postcode_udf = udf(transform_postcode, StringType())\n",
    "transform_fulladdress_udf = udf(transform_fulladdress, StringType())\n",
    "transform_expected_uprn_udf = udf(transform_expected_uprn, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b873fa4-e704-439e-895b-bc853f277b0a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get all canonical address data"
    }
   },
   "outputs": [],
   "source": [
    "# Get all GB addresses\n",
    "all_address_sdf = spark.read.option(\"header\", True).csv([\n",
    "    # \"dbfs:/Volumes/catalog-sbx-uks-ctdp-001/schema-sbx-uks-ctdp-001-acquire-clickops/test_ctdp/part_0.csv\",\n",
    "    # \"dbfs:/Volumes/catalog-sbx-uks-ctdp-001/schema-sbx-uks-ctdp-001-acquire-clickops/test_ctdp/part_1.csv\",\n",
    "    # \"dbfs:/Volumes/catalog-sbx-uks-ctdp-001/schema-sbx-uks-ctdp-001-acquire-clickops/test_ctdp/part_2.csv\",\n",
    "    \"dbfs:/Volumes/catalog-sbx-uks-ctdp-001/schema-sbx-uks-ctdp-001-acquire-clickops/test_ctdp/part_3.csv\",\n",
    "])\n",
    "\n",
    "# Extract required fields for address matching\n",
    "all_address_sdf = all_address_sdf.select(\n",
    "    \"uprn\",\n",
    "    \"fulladdress\",\n",
    "    \"postcode\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b676a51a-c273-4fcb-aefa-6dd3ff5decad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create generic test sample data"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "number_of_test_rows = 50\n",
    "\n",
    "# Generate SAMPLE of addresses to dirty\n",
    "address_samples = all_address_sdf.orderBy(F.rand()).limit(number_of_test_rows)\n",
    "address_samples = address_samples.withColumnRenamed(\"uprn\", \"expected_uprn\")\n",
    "# give each test address a key of 'unique_id' to join-on in matcher output (UPRNs want be present in all matcher output)\n",
    "address_samples = address_samples.withColumn(\"unique_id\", F.monotonically_increasing_id())\n",
    "\n",
    "address_samples = address_samples \\\n",
    "    .withColumn(\"postcode\", transform_postcode_udf(\"unique_id\", \"postcode\")) \\\n",
    "    .withColumn(\"fulladdress\", transform_fulladdress_udf(\"unique_id\", \"fulladdress\")) \\\n",
    "    .withColumn(\"expected_uprn\", transform_expected_uprn_udf(\"unique_id\", \"expected_uprn\"))\n",
    "\n",
    "count = address_samples.count()\n",
    "print(f\"Number of test data rows: {count}\")\n",
    "address_samples.show(3, truncate=False)\n",
    "# we dont want uprn potentially interfering with matching\n",
    "input_address_samples = address_samples.drop(\"expected_uprn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9208a8-7920-4fe2-82d1-779c29342413",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MoJ algorithm inputs"
    }
   },
   "outputs": [],
   "source": [
    "# # change column names to match moj address matcher's expectations\n",
    "moj_input_address_samples = input_address_samples.withColumnRenamed(\"fulladdress\", \"address_concat\")\n",
    "moj_all_address_subset_sdf = all_address_sdf.withColumnRenamed(\"uprn\", \"unique_id\").withColumnRenamed(\"fulladdress\", \"address_concat\")\n",
    "\n",
    "# MoJ algo wants to read from a Parquet file format\n",
    "moj_input_address_samples.write.mode(\"overwrite\").parquet(\"/Volumes/catalog-sbx-uks-corecdp-001/schema-sbx-uks-corecdp-acquire-001/volume-sbx-uks-corecdp-acquire-001/address_data/test/\")\n",
    "moj_all_address_subset_sdf.write.mode(\"overwrite\").parquet(\"/Volumes/catalog-sbx-uks-corecdp-001/schema-sbx-uks-corecdp-acquire-001/volume-sbx-uks-corecdp-acquire-001/address_data/all/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_address_test_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
