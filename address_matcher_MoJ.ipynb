{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f4bd58-ed5f-4bd7-a1da-589bc90a5517",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grant Permissions"
    }
   },
   "outputs": [],
   "source": [
    "%run ./grant_access_to_address_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fa2c2c-6cb9-49a1-be84-5894feda613f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "%pip install --pre uk_address_matcher duckdb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5334b52-c90c-48e3-9de5-b7b6c0e75dd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Address Data Frames (All addresses and Test addresses)"
    }
   },
   "outputs": [],
   "source": [
    "%run ./create_address_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2b02c0-ab66-4d5b-b750-1e845fda6a98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Address matcher function"
    }
   },
   "outputs": [],
   "source": [
    "# convert spark df to DuckDBimport duckdb\n",
    "import duckdb\n",
    "from uk_address_matcher import (\n",
    "    clean_data_using_precomputed_rel_tok_freq,\n",
    "    get_linker,\n",
    "    best_matches_with_distinguishability,\n",
    "    improve_predictions_using_distinguishing_tokens,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "def matcher(is_top_k_address_search, query_addresses_pdf):\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "\n",
    "    # Extract required fields for address matching\n",
    "    all_address_subset_pdf = all_address_pdf[[\n",
    "        \"uprn\",\n",
    "        \"fulladdress\",\n",
    "        \"postcode\"\n",
    "    ]].copy()\n",
    "\n",
    "    # change column names to match uk_address_matcher's expectations\n",
    "    all_address_subset_pdf.columns = [\"unique_id\", \"address_concat\", \"postcode\"]\n",
    "\n",
    "    # Register canonical (NGD) and query datasets\n",
    "    con.register(\"canonical_df\", all_address_subset_pdf)\n",
    "    con.register(\"query_df\", query_addresses_pdf)\n",
    "\n",
    "    # Clean both datasets using uk_address_matcher's pre-trained model\n",
    "    # Returns Duck DB relations\n",
    "    duckdb_canonical_clean = clean_data_using_precomputed_rel_tok_freq(con.table(\"canonical_df\"), con=con)\n",
    "    duckdb_query_clean = clean_data_using_precomputed_rel_tok_freq(con.table(\"query_df\"), con=con)\n",
    "\n",
    "    # DEBUGGING\n",
    "    # duckdb_query_clean_df = duckdb_query_clean.df()\n",
    "    # duckdb_canonical_clean_df = duckdb_canonical_clean.df()\n",
    "    # print(duckdb_canonical_clean_df.columns)\n",
    "    # print(duckdb_query_clean_df.columns)\n",
    "\n",
    "    # Create the address matcher (linker)\n",
    "    linker = get_linker(\n",
    "        df_addresses_to_match=duckdb_query_clean,\n",
    "        df_addresses_to_search_within=duckdb_canonical_clean,\n",
    "        con=con,\n",
    "        include_full_postcode_block=True,\n",
    "        additional_columns_to_retain=[\"original_address_concat\"]\n",
    "    )\n",
    "    \n",
    "    # First-pass match (good for top k matches)\n",
    "    # Returns polars df\n",
    "    df_predict = linker.inference.predict(threshold_match_weight=-50, experimental_optimisation=True)\n",
    "    # Returns Duck DB relations\n",
    "    df_predict_ddb = df_predict.as_duckdbpyrelation()\n",
    "    \n",
    "    # Second-pass refinement using distinguishing tokens\n",
    "    # Returns Duck DB relations\n",
    "    df_predict_improved = improve_predictions_using_distinguishing_tokens(\n",
    "        df_predict=df_predict_ddb,\n",
    "        con=con,\n",
    "        match_weight_threshold=5\n",
    "    )\n",
    "    \n",
    "\n",
    "    if is_top_k_address_search:\n",
    "        return df_predict_improved.df()\n",
    "    else:\n",
    "        # Get the best match per input record\n",
    "        # Returns Duck DB relations\n",
    "        best_matches = best_matches_with_distinguishability(\n",
    "            df_predict=df_predict_improved,\n",
    "            df_addresses_to_match=con.table(\"query_df\"),\n",
    "            con=con\n",
    "        )\n",
    "        return best_matches.df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a6e0b7-e542-4155-bfdd-d02b4bcc82e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call address matcher"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "address_search_with_uprn_pdf[\"unique_id\"] = address_search_with_uprn_pdf[\"unique_id\"].astype(\"Int64\")\n",
    "\n",
    "# remove UPRN from address_search_df\n",
    "address_search_pdf = address_search_with_uprn_pdf.drop(columns=[\"expected_uprn\"], errors='ignore')\n",
    "\n",
    "matched_pdf = matcher(False, address_search_pdf)\n",
    "matched_pdf = matched_pdf.rename(columns={\"unique_id_l\": \"matched_uprn\", \"unique_id_r\": \"input_unique_id\"})\n",
    "df_eval = address_search_with_uprn_pdf.merge(matched_pdf, left_on=\"unique_id\", right_on=\"input_unique_id\", how=\"left\")\n",
    "display(df_eval)\n",
    "\n",
    "# DEBUG\n",
    "# print(matched_pdf.columns)\n",
    "# print(matched_pdf.dtypes[\"unique_id\"])\n",
    "# print(address_search_pdf_with_uprn.dtypes[\"unique_id\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a407dab-2040-4bdf-9dc3-949379059c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def classify_match(row):\n",
    "\n",
    "    \"\"\"\n",
    "        TP: Correct match\n",
    "        FP: Wrong match (false positive)\n",
    "        FN: Missed a match\n",
    "        TN: Correctly left unmatched\n",
    "    \"\"\"\n",
    "    # check if null\n",
    "\n",
    "    if pd.isnull(row[\"matched_uprn\"]) and pd.isnull(row[\"expected_uprn\"]):\n",
    "        return \"TN\"\n",
    "    elif pd.notnull(row[\"matched_uprn\"]) and pd.isnull(row[\"expected_uprn\"]):\n",
    "        return \"FP\"\n",
    "    elif pd.isnull(row[\"matched_uprn\"]) and pd.notnull(row[\"expected_uprn\"]):\n",
    "        return \"FN\"\n",
    "    elif row[\"expected_uprn\"] == row[\"matched_uprn\"]:\n",
    "        return \"TP\"\n",
    "    else:\n",
    "        return \"TN\"\n",
    " \n",
    "df_eval[\"match_result\"] = df_eval.apply(classify_match, axis=1)\n",
    "#  work out match weight for each row\n",
    "df_eval[\"match_weight\"] = df_eval[\"match_weight\"].map(lambda x: round(x, 2) if pd.notnull(x) else None)\n",
    "\n",
    " \n",
    "# count metrics\n",
    "tp = (df_eval[\"match_result\"] == \"TP\").sum()\n",
    "fp = (df_eval[\"match_result\"] == \"FP\").sum()\n",
    "fn = (df_eval[\"match_result\"] == \"FN\").sum()\n",
    "match_weight = round(df_eval[\"match_weight\"].mean(),2)\n",
    "precision = round(tp / (tp + fp), 2)\n",
    "recall = round(tp / (tp + fn),2)\n",
    "f1 = round(2 * (precision * recall) / (precision + recall),2)\n",
    "distinguishability = round(df_eval[\"distinguishability\"].mean(),2)\n",
    "\n",
    "print(f\"TP: {tp}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Match weight: {match_weight}\")\n",
    "print(f\"Distinguishability: {distinguishability}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6125728636419503,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "address_matcher_MoJ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
