{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "33f4bd58-ed5f-4bd7-a1da-589bc90a5517",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grant Permissions"
    }
   },
   "outputs": [],
   "source": [
    "# %run ./grant_access_to_address_data\n",
    "# %run ./local_dev/spark_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fa2c2c-6cb9-49a1-be84-5894feda613f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "%pip install --pre uk_address_matcher duckdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2b02c0-ab66-4d5b-b750-1e845fda6a98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Address matcher function"
    }
   },
   "outputs": [],
   "source": [
    "# convert spark df to DuckDBimport duckdb\n",
    "import duckdb\n",
    "from uk_address_matcher import (\n",
    "    clean_data_using_precomputed_rel_tok_freq,\n",
    "    get_linker,\n",
    "    best_matches_with_distinguishability,\n",
    "    improve_predictions_using_distinguishing_tokens,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "def matcher(is_top_k_address_search):\n",
    "    \"\"\"\n",
    "    by default top_k is set to 5 in the MoJ algorithm\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    p_input = \"/dbfs/tmp/address_data/input/*.parquet\"\n",
    "    p_all = \"/dbfs/tmp/address_data/all/*.parquet\"\n",
    "\n",
    "    duck_df_input = con.read_parquet(p_input).order(\"postcode\")\n",
    "    duck_df_all = con.read_parquet(p_all).order(\"postcode\")\n",
    "\n",
    "    # Clean both datasets using uk_address_matcher's pre-trained model\n",
    "    # Returns Duck DB relations\n",
    "    duckdb_canonical_clean = clean_data_using_precomputed_rel_tok_freq(duck_df_all, con=con)\n",
    "    duckdb_query_clean = clean_data_using_precomputed_rel_tok_freq(duck_df_input, con=con)\n",
    "\n",
    "    # DEBUGGING\n",
    "    # duckdb_query_clean_df = duckdb_query_clean.df()\n",
    "    # duckdb_canonical_clean_df = duckdb_canonical_clean.df()\n",
    "    # print(duckdb_canonical_clean_df.columns)\n",
    "    # print(duckdb_query_clean_df.columns)\n",
    "\n",
    "    # Create the address matcher (linker)\n",
    "    linker = get_linker(\n",
    "        df_addresses_to_match=duckdb_query_clean,\n",
    "        df_addresses_to_search_within=duckdb_canonical_clean,\n",
    "        con=con,\n",
    "        include_full_postcode_block=True,\n",
    "        additional_columns_to_retain=[\"original_address_concat\"]\n",
    "    )\n",
    "    \n",
    "    # First-pass match (good for top k matches)\n",
    "    # Returns polars df\n",
    "    df_predict = linker.inference.predict(threshold_match_weight=-50, experimental_optimisation=True)\n",
    "    # Returns Duck DB relations\n",
    "    df_predict_ddb = df_predict.as_duckdbpyrelation()\n",
    "    \n",
    "    # Second-pass refinement using distinguishing tokens\n",
    "    # Returns Duck DB relations\n",
    "    df_predict_improved = improve_predictions_using_distinguishing_tokens(\n",
    "        df_predict=df_predict_ddb,\n",
    "        con=con,\n",
    "        match_weight_threshold=-20\n",
    "    )\n",
    "    \n",
    "\n",
    "    if is_top_k_address_search:\n",
    "        return df_predict_improved.df()\n",
    "    else:\n",
    "        # Get the best match per input record\n",
    "        # Returns Duck DB relations\n",
    "        best_matches = best_matches_with_distinguishability(\n",
    "            df_predict=df_predict_improved,\n",
    "            df_addresses_to_match=duck_df_input,\n",
    "            con=con\n",
    "        )\n",
    "        return best_matches.df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5334b52-c90c-48e3-9de5-b7b6c0e75dd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Address Data (canonical addresses and test addresses)"
    }
   },
   "outputs": [],
   "source": [
    "%run ./create_address_test_data\n",
    "# creates 'address_samples' used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a6e0b7-e542-4155-bfdd-d02b4bcc82e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call address matcher"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# call address matcher\n",
    "matched_sdf = spark.createDataFrame(matcher(False)).withColumnRenamed(\"unique_id_l\", \"matched_uprn\").withColumnRenamed(\"unique_id_r\", \"test_address_input_id\").withColumnRenamed(\"original_address_concat\", \"address_concat_l\")\n",
    "# merge matcher outputs with address_samples_dirty\n",
    "df_eval = matched_sdf.join(address_samples, address_samples.unique_id == matched_sdf.test_address_input_id, \"left\")\n",
    "# display top 10 rows\n",
    "df_eval.show(10, truncate=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a407dab-2040-4bdf-9dc3-949379059c93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1-1 matching metrics"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round as spark_round, mean\n",
    "\n",
    "def classify_match(df):\n",
    "    return df.withColumn(\n",
    "        \"match_result\",\n",
    "        when(col(\"matched_uprn\").isNull() & col(\"expected_uprn\").isNull(), \"TN\")\n",
    "        .when(col(\"matched_uprn\").isNotNull() & col(\"expected_uprn\").isNull(), \"FP\")\n",
    "        .when(col(\"matched_uprn\").isNull() & col(\"expected_uprn\").isNotNull(), \"FN\")\n",
    "        .when(col(\"expected_uprn\") == col(\"matched_uprn\"), \"TP\")\n",
    "        .when(col(\"expected_uprn\") != col(\"matched_uprn\"), \"FP\")\n",
    "        .otherwise(\"TN\")\n",
    "    )\n",
    "\n",
    "df_eval = classify_match(df_eval)\n",
    "\n",
    "df_eval = df_eval.withColumn(\n",
    "    \"match_weight\",\n",
    "    when(col(\"match_weight\").isNotNull(), spark_round(col(\"match_weight\"), 2))\n",
    ")\n",
    "\n",
    "# count metrics\n",
    "tp = df_eval.filter(col(\"match_result\") == \"TP\").count()\n",
    "fp = df_eval.filter(col(\"match_result\") == \"FP\").count()\n",
    "fn = df_eval.filter(col(\"match_result\") == \"FN\").count()\n",
    "tn = df_eval.filter(col(\"match_result\") == \"TN\").count()\n",
    "\n",
    "# display 'matched_uprn' and 'expected_uprn' columns for FPs and FNs\n",
    "if fp > 0:\n",
    "    print(\"FP List\")\n",
    "    display(df_eval.filter(col(\"match_result\") == \"FP\").select(\"address_concat_r\", \"original_address_concat_l\", \"matched_uprn\", \"expected_uprn\", \"match_weight\", \"match_result\"))\n",
    "if fn > 0:\n",
    "    print(\"FN List\")\n",
    "    display(df_eval.filter(col(\"match_result\") == \"FN\").select(\"address_concat_r\", \"original_address_concat_l\", \"matched_uprn\", \"expected_uprn\", \"match_weight\", \"match_result\"))\n",
    "\n",
    "match_weight = df_eval.select(spark_round(mean(\"match_weight\"), 2)).collect()[0][0]\n",
    "precision = round(tp / (tp + fp) if (tp + fp) != 0 else 0, 2)\n",
    "recall = round(tp / (tp + fn), 2)\n",
    "f1 = round(2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0, 2)\n",
    "distinguishability = df_eval.select(spark_round(mean(\"distinguishability\"), 2)).collect()[0][0]\n",
    "print(\"Predictive successes:\")\n",
    "print(f\"TP/True Postitive: Model correctly predicted a match.\")\n",
    "print(tp)\n",
    "print(f\"TN/True Negative: Model correctly predicted no match.\")\n",
    "print(tn)\n",
    "print(\"Predictive errors:\")\n",
    "print(f\"FP/False Positive: Model predicted a match when there were none (or predicted the wrong match).\")\n",
    "print(fp)\n",
    "print(f\"FN/False Negative: Model predicted no match, when there was one.\")\n",
    "print(fn)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Match weight: {match_weight}\")\n",
    "print(f\"Distinguishability: {distinguishability}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "address_matcher_MoJ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
